{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from datetime import timedelta\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import mplleaflet\n",
    "import math\n",
    "\n",
    "\n",
    "##### IMPORTANT METHODS DEFINITION #####\n",
    "\n",
    "def get_angle(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Returns angle between the coordinates\n",
    "    \"\"\"\n",
    "    lon_diff = lon2 - lon1\n",
    "    lat_diff = lat2 - lat1\n",
    "    return np.arctan2(lat_diff, lon_diff)\n",
    "\n",
    "def get_angle_degrees(lon1, lat1, lon2, lat2):\n",
    "    return np.degrees(get_angle(lon1, lat1, lon2, lat2))\n",
    "\n",
    "def get_angle_points(pt1: Point, pt2: Point):\n",
    "    \"\"\"\n",
    "    Returns angle between two points\n",
    "    http://wikicode.wikidot.com/get-angle-of-line-between-two-points\n",
    "    x is lon, y is lat\n",
    "    \"\"\"\n",
    "    lon_diff = pt2.x - pt1.x\n",
    "    lat_diff = pt2.y - pt1.y\n",
    "    return math.degrees(math.atan2(lat_diff, lon_diff))\n",
    "\n",
    "def get_angle_line_string(line_string: LineString):\n",
    "    \"\"\"\n",
    "    Returns the angle between the first two points in a linestring\n",
    "    \"\"\"\n",
    "    return get_angle(line_string[0],line_string[1])\n",
    "\n",
    "def points_to_linestring(points):\n",
    "    \"\"\"\n",
    "    The first given point will be the first in order\n",
    "    \"\"\"\n",
    "    points_tuple_list = []\n",
    "    for point in points:\n",
    "        points_tuple_list.append((point.x, point.y))\n",
    "    return LineString(points_tuple_list)\n",
    "\n",
    "def parse_brazilian_datetime(dt_series):\n",
    "    return pd.datetime.strptime(str(dt_series), '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# IMPORTANT VARIABLES DEFINITION\n",
    "\n",
    "folder_path = 'G:/onibus-curitiba-2018-10/'\n",
    "chosen_bus_lines = ['022']\n",
    "chosen_vehicles = ['LR802']\n",
    "gps_datum = {'init': 'epsg:4326'} # Distance in degrees\n",
    "sirgas_utm_22s_datum = {'init': 'epsg:31982'} # Distance in meters\n",
    "buffer_radius = 30\n",
    "time_dif_seconds_threshold = 600\n",
    "fig = None\n",
    "\n",
    "##### CHECKING FILES IN FILESYSTEM #####\n",
    "\n",
    "all_files = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "\n",
    "tables = [\n",
    "    'linhas',\n",
    "    'pontosLinha',\n",
    "    'shapeLinha',\n",
    "    'tabelaVeiculo',\n",
    "    'veiculos'\n",
    "]\n",
    "\n",
    "files = pd.DataFrame({'file_name':all_files})\n",
    "files['file_path'] = files.apply(lambda row: folder_path + row['file_name'], axis=1)\n",
    "files['date'] = files.apply(lambda row: np.datetime64(re.search(r'[0-9]{4}_[0-9]{2}_[0-9]{2}',row['file_name']).group().replace('_','-'), 'D'), axis=1)\n",
    "files['file_type'] = files.apply(lambda row: re.search(r'(.+_)([a-zA-Z]+)(\\.json\\.xz)',row['file_name']).group(2), axis=1)\n",
    "files = files.astype({'file_path':'category','file_type':'category','file_name':'category'})\n",
    "files.sort_values(by=['file_type','date',],inplace=True)\n",
    "\n",
    "files_filtered_by_date = files[(files['date'] >= '2018-10-02') & (files['date'] <= '2018-10-02')]\n",
    "files_filtered_by_date.set_index(\n",
    "    keys=['file_type','date'],\n",
    "    drop=False,\n",
    "    append=False,\n",
    "    inplace=True,\n",
    "    verify_integrity=True\n",
    ")\n",
    "files_filtered_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### VEICULOS #####\n",
    "\n",
    "columns_rename = {\n",
    "    'COD_LINHA': 'line_id',\n",
    "    'DTHR': 'date_time',\n",
    "    'LAT': 'lat',\n",
    "    'LON': 'lon',\n",
    "    'VEIC': 'vehicle_id'\n",
    "}\n",
    "\n",
    "columns_types = {\n",
    "    'lat': 'float64',\n",
    "    'lon': 'float64',\n",
    "    'vehicle_id': 'category'\n",
    "}\n",
    "\n",
    "filtered_files = files_filtered_by_date[files_filtered_by_date['file_type'] == 'veiculos']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "print(\"Using the following files:\")\n",
    "for index, row in filtered_files.iterrows():\n",
    "    file = row['file_path']\n",
    "    print(file)\n",
    "    date = row['date']\n",
    "    df = pd.read_json(lines=True,path_or_buf=file)\n",
    "    print('Length before filtering {}'.format(len(df.index)))\n",
    "    df['COD_LINHA'] = df['COD_LINHA'].apply(str)\n",
    "    df = df[df['COD_LINHA'].isin(chosen_bus_lines)]\n",
    "    df = df[df['VEIC'].isin(chosen_vehicles)]\n",
    "    df['date'] = date\n",
    "    df_list.append(df)\n",
    "    print('Length after filtering {}'.format(len(df.index)))\n",
    "\n",
    "veiculos = pd.concat(df_list).rename(index=str, columns=columns_rename).astype(columns_types)\n",
    "\n",
    "\n",
    "veiculos.line_id = veiculos.line_id.apply(str)\n",
    "veiculos.line_id = veiculos.line_id.astype('category')\n",
    "veiculos['date_time'] = veiculos['date_time'].apply(parse_brazilian_datetime)\n",
    "print(veiculos.dtypes)\n",
    "veiculos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('After line filtering')\n",
    "veiculos['coordinates'] = veiculos.apply(lambda row: Point(row['lon'], row['lat']), axis=1)\n",
    "veiculos = gpd.GeoDataFrame(veiculos, geometry='coordinates', crs=gps_datum)\n",
    "veiculos = veiculos.to_crs(sirgas_utm_22s_datum)\n",
    "veiculos['date_time_shifted'] = veiculos.groupby(by=['line_id','vehicle_id'])['date_time'].shift(1)\n",
    "veiculos['coordinates_shifted'] = veiculos.groupby(by=['line_id','vehicle_id'])['coordinates'].shift(1)\n",
    "veiculos.dropna(axis=0, how='any', inplace=True)\n",
    "veiculos['line_string'] = veiculos.apply(lambda row: points_to_linestring([row['coordinates'],row['coordinates_shifted']]), axis=1)\n",
    "veiculos = veiculos.set_geometry('line_string')\n",
    "veiculos['line_string_length'] = veiculos['line_string'].length\n",
    "veiculos['time_dif'] = veiculos['date_time_shifted'] - veiculos['date_time']\n",
    "veiculos['km_per_hour'] = (veiculos['line_string_length']/1000)/ (veiculos['time_dif'].dt.total_seconds()/3600)\n",
    "print(veiculos.dtypes)\n",
    "print('km_per_hour analysis:\\n {}\\n'.format(veiculos.km_per_hour.describe()))\n",
    "print('distance analysis:\\n {}\\n'.format(veiculos.line_string_length.describe()))\n",
    "veiculos = veiculos[['date','line_id','vehicle_id','date_time','line_string','line_string_length','time_dif','km_per_hour']]\n",
    "veiculos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selected_bus_line in chosen_bus_lines:\n",
    "    print('The vehicles operating in the line {} are {}.'\n",
    "          .format(\n",
    "              selected_bus_line,\n",
    "              veiculos.vehicle_id[veiculos.line_id == selected_bus_line].unique()\n",
    "          )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = veiculos['line_string'].plot(edgecolor='red', color='red')\n",
    "print('The Map')\n",
    "mplleaflet.show(fig=fig.figure, crs=sirgas_utm_22s_datum, tiles='cartodb_positron', path='hello.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### LINHAS ######\n",
    "columns_rename = {\n",
    "    'CATEGORIA_SERVICO':'category',\n",
    "    'COD': 'line_id',\n",
    "    'NOME': 'line_name',\n",
    "    'NOME_COR': 'color',\n",
    "    'SOMENTE_CARTAO': 'only_card'\n",
    "}\n",
    "\n",
    "columns_types = {\n",
    "    'category': 'category',\n",
    "    'line_id': 'category',\n",
    "    'line_name': 'category',\n",
    "    'color': 'category',\n",
    "    'only_card': 'category'\n",
    "}\n",
    "\n",
    "filtered_files = files_filtered_by_date[files_filtered_by_date['file_type'] == 'linhas']\n",
    "\n",
    "df_list = []\n",
    "print(\"Using the following files:\")\n",
    "for index, row in filtered_files.iterrows():\n",
    "    file = row['file_path']\n",
    "    print(file)\n",
    "    date = row['date']\n",
    "    df = pd.read_json(orient='records',path_or_buf=file)\n",
    "    df['date'] = date\n",
    "    df['COD'] = df['COD'].apply(str)\n",
    "    df_list.append(df)\n",
    "    \n",
    "linhas = pd.concat(df_list).rename(index=str, columns=columns_rename).astype(columns_types)\n",
    "\n",
    "# linhas.reset_index(level=-1, inplace=True)\n",
    "'''\n",
    "linhas.set_index(\n",
    "    keys=['date','line_id'],\n",
    "    drop=False,\n",
    "    append=False,\n",
    "    inplace=True,\n",
    "    verify_integrity=True\n",
    ")\n",
    "'''\n",
    "\n",
    "linhas = linhas[linhas['line_id'].isin(chosen_bus_lines)]\n",
    "linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PONTOS LINHA #####\n",
    "\n",
    "columns_rename = {\n",
    "    'COD':'line_id',\n",
    "    'GRUPO': 'group_id',\n",
    "    'ITINERARY_ID': 'itinerary_id',\n",
    "    'LAT': 'lat',\n",
    "    'LON': 'lon',\n",
    "    'NOME': 'stop_name',\n",
    "    'NUM': 'stop_id',\n",
    "    'SENTIDO': 'way',\n",
    "    'SEQ': 'stop_sequence',\n",
    "    'TIPO': 'building'\n",
    "}\n",
    "\n",
    "columns_types = {\n",
    "    'line_id': 'category',\n",
    "    'group_id': 'category',\n",
    "    'itinerary_id': 'category',\n",
    "    'lat': 'float64',\n",
    "    'lon': 'float64',\n",
    "    'stop_name': 'category',\n",
    "    'stop_id': 'object',\n",
    "    'way': 'category',\n",
    "    'stop_sequence': 'uint8',\n",
    "    'building': 'category'\n",
    "}\n",
    "\n",
    "filtered_files = files_filtered_by_date[files_filtered_by_date['file_type'] == 'pontosLinha']\n",
    "\n",
    "df_list = []\n",
    "print(\"Using the following files:\")\n",
    "for index, row in filtered_files.iterrows():\n",
    "    file = row['file_path']\n",
    "    print(file)\n",
    "    date = row['date']\n",
    "    df = pd.read_json(orient='records',path_or_buf=file)\n",
    "    df['date'] = date\n",
    "    df['COD'] = df['COD'].apply(str)\n",
    "    df_list.append(df)\n",
    "    \n",
    "pontosLinha = pd.concat(df_list).rename(index=str, columns=columns_rename)\n",
    "pontosLinha = pontosLinha.astype(columns_types)\n",
    "pontosLinha['stop_id'] = pontosLinha['stop_id'].apply(str)\n",
    "pontosLinha['stop_id'] = pontosLinha['stop_id'].astype('category')\n",
    "\n",
    "'''\n",
    "pontosLinha.set_index(\n",
    "    keys=['date','line_id','itinerary_id','stop_sequence','way'],\n",
    "    drop=False,\n",
    "    append=False,\n",
    "    inplace=True,\n",
    "    verify_integrity=True\n",
    ")\n",
    "'''\n",
    "\n",
    "pontosLinha = pontosLinha[pontosLinha['line_id'].isin(chosen_bus_lines)]\n",
    "\n",
    "pontosLinha.sort_values(by=['date','line_id','itinerary_id','way','stop_sequence',],inplace=True)\n",
    "pontosLinha['stop_coord'] = pontosLinha.apply(lambda row: Point(row['lon'], row['lat']), axis=1)\n",
    "pontosLinha = gpd.GeoDataFrame(pontosLinha, geometry='stop_coord', crs=gps_datum)\n",
    "pontosLinha = pontosLinha.to_crs(sirgas_utm_22s_datum)\n",
    "pontosLinha = pontosLinha.reindex(columns=['date','line_id','itinerary_id','way','stop_sequence','stop_id','stop_coord','stop_name','building','group_id'])\n",
    "pontosLinha['stop_buffer'] = pontosLinha['stop_coord'].buffer(buffer_radius)\n",
    "uniqueItineraries = pontosLinha['itinerary_id'].unique()\n",
    "print(uniqueItineraries)\n",
    "#print('Counting frequency of stop_ids:')\n",
    "#print(pontosLinha['stop_id'].value_counts())\n",
    "bus_stops = pontosLinha.drop_duplicates(subset=['date','line_id','stop_id'], keep='first', inplace=False)\n",
    "bus_stops = bus_stops[bus_stops['line_id'].isin(chosen_bus_lines)]\n",
    "#bus_stops = bus_stops[['date','line_id','stop_id','stop_coord','stop_name','building','group_id','stop_buffer']]\n",
    "print('Number of repeated rows by {}\\n************************'.format(bus_stops.groupby(['line_id','itinerary_id','stop_sequence','stop_id'],as_index=False).size()))\n",
    "bus_stops.drop_duplicates(subset=['line_id','itinerary_id','stop_sequence','stop_id'],keep='first',inplace=True)\n",
    "bus_stops.drop(labels=['date','line_id','itinerary_id','way','group_id'],axis=1,inplace=True)\n",
    "bus_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stops = gpd.GeoDataFrame(bus_stops, geometry='stop_buffer', crs=sirgas_utm_22s_datum)\n",
    "fig = bus_stops['stop_buffer'].plot(ax=fig, edgecolor='green', color='green')\n",
    "# Displaying Map\n",
    "print('The Map')\n",
    "mplleaflet.show(fig=fig.figure, crs=sirgas_utm_22s_datum, tiles='cartodb_positron',path='stops-path.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_over_stop = gpd.sjoin(veiculos, bus_stops, how='inner', op='intersects', lsuffix='vehi', rsuffix='stop')\n",
    "vehicle_over_stop.head()\n",
    "\n",
    "# Fazer loop sobre vehicles separando por date e vehicle_id\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_over_stop = gpd.GeoDataFrame(vehicle_over_stop, geometry='line_string', crs=sirgas_utm_22s_datum)\n",
    "vehicle_over_stop['distance_from_stop'] = gpd.GeoSeries(vehicle_over_stop['line_string'], crs=sirgas_utm_22s_datum).distance(gpd.GeoSeries(vehicle_over_stop['stop_coord'], crs=sirgas_utm_22s_datum))\n",
    "vehicle_over_stop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_over_stop['date_time_group'] = np.uint16(0)\n",
    "date_time_column_id = vehicle_over_stop.columns.get_loc('date_time')\n",
    "date_time_group_column_id = vehicle_over_stop.columns.get_loc('date_time_group')\n",
    "group_id = np.uint16(0)\n",
    "for i in range(0, len(vehicle_over_stop)):\n",
    "    try:\n",
    "        # Pega o tempo anterior\n",
    "        # Compara com o atual\n",
    "        # Se a diferenÃ§a dos dois for \n",
    "        # menor que certa threshold, seta o grupo group_id nela\n",
    "        # maior que certa threshold, atualiza group_id + 1 e seta o group_id nela\n",
    "        previous_date_time = vehicle_over_stop.iat[i-1, date_time_column_id]\n",
    "        current_date_time = vehicle_over_stop.iat[i, date_time_column_id]\n",
    "        absolute_time_dif_seconds = np.absolute(current_date_time - previous_date_time)/np.timedelta64(1, 's')\n",
    "        # print('Absolute time dif {}'.format(absolute_time_dif_seconds))\n",
    "        if absolute_time_dif_seconds > time_dif_seconds_threshold:\n",
    "            group_id = group_id + 1\n",
    "        vehicle_over_stop.iat[i, date_time_group_column_id] = group_id\n",
    "        # print('Group id {}'.format(group_id))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "vehicle_over_stop\n",
    "vehicle_over_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_over_stop_grouped = vehicle_over_stop.loc[vehicle_over_stop.groupby('date_time_group')['distance_from_stop'].idxmin()]\n",
    "vehicle_over_stop_grouped.sort_values(by=['date_time'],inplace=True)\n",
    "vehicle_over_stop_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and cleaning table shapeLinha\n",
    "\n",
    "columns_rename = {\n",
    "    'COD': 'line_id',\n",
    "    'LAT': 'lat',\n",
    "    'LON': 'lon',\n",
    "    'SHP': 'shape_id'\n",
    "}\n",
    "\n",
    "columns_types = {\n",
    "    'line_id': 'category',\n",
    "    'lat': 'float64',\n",
    "    'lon': 'float64',\n",
    "    'shape_id': 'uint32'\n",
    "}\n",
    "\n",
    "filtered_files = files_filtered_by_date[files_filtered_by_date['file_type'] == 'shapeLinha']\n",
    "\n",
    "df_list = []\n",
    "print(\"Using the following files:\")\n",
    "for index, row in filtered_files.iterrows():\n",
    "    file = row['file_path']\n",
    "    print(file)\n",
    "    date = row['date']\n",
    "    df = pd.read_json(orient='records',path_or_buf=file)\n",
    "    df['date'] = date\n",
    "    df['COD'] = df['COD'].apply(str)\n",
    "    # Changes the index name to point_sequence, because the order the points are presented in the archive matters\n",
    "    df.index.name = 'point_sequence'\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df_list.append(df)\n",
    "    \n",
    "shapeLinha = pd.concat(df_list).rename(index=str, columns=columns_rename).astype(columns_types)\n",
    "\n",
    "\n",
    "\n",
    "shapeLinha = shapeLinha[shapeLinha['line_id'].isin(chosen_bus_lines)]\n",
    "\n",
    "# Creating the georefferenced shape\n",
    "shapeLinha['coordinates'] = shapeLinha.apply(lambda row: Point(row['lon'], row['lat']), axis=1)\n",
    "\n",
    "# Gets a list of unique shape_ids\n",
    "unique_shapes = shapeLinha['shape_id'].unique()\n",
    "# Gets a list of unique line_ids\n",
    "unique_lines = shapeLinha['line_id'].unique()\n",
    "# Gets a list of unique dates\n",
    "unique_dates = shapeLinha['date'].unique()\n",
    "\n",
    "shapeLinha['coordinates_shifted'] = shapeLinha.groupby(by=['date','line_id','shape_id'])['coordinates'].shift(1)\n",
    "\n",
    "# Deleting rows with NaN \n",
    "shapeLinha.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Creating line_string column\n",
    "shapeLinha['line_string'] = shapeLinha.apply(lambda row: points_to_linestring([row['coordinates'],row['coordinates_shifted']]), axis=1)\n",
    "shapeLinha = gpd.GeoDataFrame(shapeLinha, geometry='line_string', crs=gps_datum)\n",
    "shapeLinha = shapeLinha.to_crs(sirgas_utm_22s_datum)\n",
    "shapeLinha['distance'] = shapeLinha.geometry.length\n",
    "print('Number of unique shapes {}. The shape ids are: {}.'.format(len(unique_shapes), unique_shapes))\n",
    "print('Number of unique dates {}. The dates are {}'.format(len(unique_dates),unique_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Plot\n",
    "plotColors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "# Creating the index\n",
    "'''\n",
    "shapeLinha.set_index(\n",
    "    keys=['date','line_id','shape_id','point_sequence'],\n",
    "    drop=False,\n",
    "    append=False,\n",
    "    inplace=True,\n",
    "    verify_integrity=True\n",
    ")\n",
    "'''\n",
    "\n",
    "# Filtering using indexes\n",
    "# fig = shapeLinha.loc['2018-10-01',chosen_bus_line,2778,:].plot(edgecolor='r', color='r')\n",
    "# fig = shapeLinha.loc['2018-10-01',chosen_bus_line,1785,:].plot(ax=fig, edgecolor='b', color='b')\n",
    "# fig = shapeLinha.loc['2018-10-02',chosen_bus_line,2778,:].plot(ax=fig, edgecolor='g', color='g')\n",
    "# Filtering using regular columns\n",
    "count = 0\n",
    "for date in unique_dates:\n",
    "    for line in unique_lines:\n",
    "        for shape in unique_shapes:\n",
    "            current_color = 'C'+str(count)\n",
    "            if fig is None:\n",
    "                fig = shapeLinha[\n",
    "                    (shapeLinha['date'] == date) & \n",
    "                    (shapeLinha['line_id'] == line) & \n",
    "                    (shapeLinha['shape_id'] == shape)\n",
    "                ].plot(edgecolor=current_color, color=current_color)\n",
    "            else:\n",
    "                fig = shapeLinha[\n",
    "                    (shapeLinha['date'] == date) & \n",
    "                    (shapeLinha['line_id'] == line) & \n",
    "                    (shapeLinha['shape_id'] == shape)\n",
    "                ].plot(ax=fig,edgecolor=current_color, color=current_color)\n",
    "            count = count + 1\n",
    "            print('Date {}, Line {}, Shape {} is on the map with the color {}'.format(date, line, shape, current_color))\n",
    "\n",
    "print('{} shapes were plotted on the map.'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Map\n",
    "#print('The Map')\n",
    "#mplleaflet.display(fig=fig.figure, crs=shapeLinha.crs, tiles='cartodb_positron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela Veiculo\n",
    "\n",
    "''' NOT USED! DID NOT WORK USING APPLY IN THE DATAFRAME\n",
    "def fix_dates(str_date, str_time, time_threshold):\n",
    "    \"\"\"\n",
    "    Returns a datetime64 datetime with the concatenation of str_date and str_time\n",
    "    str_date is the date as a string\n",
    "    str_time is a time as a string\n",
    "    time_threshold is a time limit. str_time is lower than the threshold, adds 1 day to the str_date\n",
    "    \"\"\"\n",
    "    pdate = datetime.strptime(str_date, '%Y-%m-%d').date() \n",
    "    ptime = datetime.strptime(str_time, '%H:%M').time()\n",
    "    print('pdate {} ptime {} time threshold {}'.format(pdate, ptime, time_threshold))\n",
    "    if ptime < time_threshold:\n",
    "        pdate = pdate + timedelta(days=1)\n",
    "    return np.datetime64(datetime.combine(pdate, ptime))\n",
    "'''\n",
    "\n",
    "columns_rename = {\n",
    "    'COD_LINHA': 'line_id',\n",
    "    'COD_PONTO': 'stop_id',\n",
    "    'HORARIO': 'scheduled_time',\n",
    "    'NOME_LINHA': 'line_name',\n",
    "    'TABELA': 'route_table',\n",
    "    'VEICULO': 'vehicle_id'\n",
    "}\n",
    "\n",
    "columns_types = {\n",
    "    'line_id': 'category',\n",
    "    'stop_id': 'category',\n",
    "    'line_name': 'category',\n",
    "    'route_table': 'category',\n",
    "    'vehicle_id': 'category'\n",
    "}\n",
    "\n",
    "filtered_files = files_filtered_by_date[files_filtered_by_date['file_type'] == 'tabelaVeiculo']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "print(\"Using the following files:\")\n",
    "for index, row in filtered_files.iterrows():\n",
    "    file = row['file_path']\n",
    "    print(file)\n",
    "    date = row['date']\n",
    "    df = pd.read_json(orient='records',path_or_buf=file)\n",
    "    df['date'] = date\n",
    "    df['COD_LINHA'] = df['COD_LINHA'].apply(str)\n",
    "    df_list.append(df)\n",
    "    \n",
    "tabelaVeiculo = pd.concat(df_list).rename(index=str, columns=columns_rename).astype(columns_types)\n",
    "\n",
    "unique_stop_ids = tabelaVeiculo['stop_id'].unique()\n",
    "\n",
    "tabelaVeiculo = tabelaVeiculo[tabelaVeiculo['line_id'].isin(chosen_bus_lines)]\n",
    "tabelaVeiculo = tabelaVeiculo[tabelaVeiculo['vehicle_id'].isin(chosen_vehicles)]\n",
    "\n",
    "first_day = np.datetime64('1900-01-01')\n",
    "tabelaVeiculo['fixed_scheduled_time'] = pd.to_datetime(\n",
    "    tabelaVeiculo['scheduled_time'], \n",
    "    format='%H:%M'\n",
    ").apply(\n",
    "    lambda x: x - first_day\n",
    ")\n",
    "tabelaVeiculo['fixed_scheduled_time'] = tabelaVeiculo['fixed_scheduled_time'] + tabelaVeiculo['date']\n",
    "\n",
    "print(tabelaVeiculo.dtypes)\n",
    "\n",
    "tabelaVeiculo = tabelaVeiculo[['date','line_id','vehicle_id','fixed_scheduled_time','scheduled_time','stop_id','route_table']]\n",
    "print('Number of repeated rows by {}\\n************************'.format(tabelaVeiculo.groupby(['line_id','vehicle_id','stop_id','scheduled_time','route_table'],as_index=False).size()))\n",
    "tabelaVeiculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_over_stop_grouped.index.names = ['vos_index']\n",
    "vehicle_over_stop_grouped.reset_index(inplace=True)\n",
    "vehicle_over_stop_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_vos = pd.merge(\n",
    "    vehicle_over_stop_grouped,tabelaVeiculo,\n",
    "    how='inner',\n",
    "    left_on=['stop_id'],\n",
    "    right_on=['stop_id'],\n",
    "    copy=False\n",
    ")\n",
    "\n",
    "tv_no_schedule = vehicle_over_stop_grouped[vehicle_over_stop_grouped.stop_id.isin(tabelaVeiculo.stop_id)]\n",
    "\n",
    "print(tv_vos.dtypes)\n",
    "print('--')\n",
    "print(tv_no_schedule.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_vos['t_scheduled'] = tv_vos['fixed_scheduled_time'].dt.time.apply(lambda x: pd.Timedelta(str(x)))\n",
    "tv_vos['t_real'] = tv_vos['date_time'].dt.time.apply(lambda x: pd.Timedelta(str(x)))\n",
    "tv_vos['delay_amount'] =  tv_vos.t_real - tv_vos.t_scheduled\n",
    "tv_vos['abs_delay_amount'] =  np.abs(tv_vos['delay_amount'])\n",
    "tv_vos.sort_values(by=['vos_index','abs_delay_amount'],ascending=True,inplace=True,na_position='last')\n",
    "tv_vos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tv_vos = tv_vos.loc[tv_vos.groupby(['vos_index'])['abs_delay_amount'].idxmin()]\n",
    "clean_tv_vos[['vos_index','date_x','date_time','t_real','t_scheduled','delay_amount','abs_delay_amount','distance_from_stop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tv_vos[['vos_index','date_x','date_time','t_real','t_scheduled','delay_amount','abs_delay_amount','distance_from_stop']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.DataFrame({\n",
    "    'dt1':['2018-01-01 12:22','2018-01-01 23:22','2018-01-01 12:00','2018-01-01 11:22'],\n",
    "    'dt2':['2018-01-01 12:26','2018-01-01 23:42','2018-01-01 12:20','2018-01-01 11:52']\n",
    "})\n",
    "tt['dt1'] = pd.to_datetime(tt['dt1'])\n",
    "tt['dt2'] = pd.to_datetime(tt['dt2'])\n",
    "tt['dt'] = tt['dt1'].dt.time\n",
    "tt['x'] = tt['dt1'].dt.time.apply(lambda x: pd.Timedelta(str(x)))\n",
    "print(tt.dtypes)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
